## Router

Router is responsible for identifying local-indexer endpoints that should
receive secondary keys from projector. On one side it talks to projector
to get KeyVersions for a vbucket, and on the other side it is connected with
local indexer nodes that will persist KeyVersions on durable storage.

Router is part of projector process, hence it uses projector's admin interface
to get IndexTopology.

* manages per vbucket input queue which is populated with projector's
  KeyVersions.
* uses hash-partition or key-partition. Partitioning algorithm is pluggable
  and partitioning algorithms like range partition and adaptive partition will
  be added at later point.
* partition algorithm maps a secondary key to slice-no and locates the key's
  shard and slice.
* `topic` and `IndexTopology` is used to identify subscribed local-indexer-node
  hosting the shard containing that slice.
* manages a streaming connection with local indexer nodes to push Mutation
  events to them.

## Index topology

Index-topology is received from stream request via projector. Topology
information within stream-request should provide correct endpoints for
publishing the KeyVersions, for instance, an indexer node might be listening
on three different ports for "maintenance", "backfill", "catchup" streams.
Topology should also include replica indexers as well.

* call a partition-algorithm with key-version. partition-algorithm can be
  backed by static tables or backed by tables that are continuously updated
  based on heuristics.
* partition algorithm is abstracted by TopologyProvider interface, each
  index can have different topology constraints and partition algorithm, there
  by giving a uniform interface for the router.
* router can locate the shard that contains the key, where each shard is
  hosted by a local indexer node.

### Topics and subscribers

A topic is created for every stream request, and subscribers are identified
through Topology interface APIs.

Following is a sample set of topics that Indexers and Index-Coordinator can
create,

1. **/maintenance**, started by Index-Coordinator.
2. **/backfill/<id>**, started by Index-Coordinator. In future releases,
   secondary index system might allow more than one backfill streams at the same
   time. `id` is generated by Index-Coordinator to differentiate between backfill
   streams.
3. **/catchup/<indexerId>**, will be started by local indexer node for
   catchup-streams. `indexerId` identifies the local indexer node starting the
   stream.

Typical flow of topic creation and subscription is as follows,

1. indexer / coordinator request a stream to projector, associating a topic to it.
2. projector will launch a router thread for each stream and pass on the
   topology information to new router thread.

### flow control between router and indexer

Router will publish mutations to indexer endpoints. Router client can open more
than one connection for the same endpoint to speed up mutation transport.

Indexer will have a server thread listening on its endpoint and router will
connect with indexer's endpoint based on the supplied topology information
from the stream-request.

If a connection drops between indexer and router, router will retry a new
connection with the endpoint until the stream is shutdown from the projector
side. Additionally, stream request will specify a flag whether to retry
downstream connection forever. In case of "catchup streams", respective
indexer node shall shutdown / restart the stream with projector.

Router will have an output queue for each endpoint and when ever the queue
overflows it will cascade to upstream buffers and eventually will throttle the
UPR streams.

This can lead to system level (system is secondary index) slow down, actually
the system level slow down is one of the many corner case. If the connection is
open but router is not able to flush the mutation out to the other end, it is
because the indexer node is slow. Again, it is possible that only this node is
slow, or most of the other nodes are slow as well, and reason for slowness can
be due to many things starting from heterogeneous indexer nodes, uneven data
distribution to faulty network cables.

The default behavior would be to allow a slow indexer node's slowness to
cascade to upstream connections leading to system wide slowness.

#### drop indicators

Since router will know when the output queue for an endpoint will overflow, it
can drop mutations when the buffer is _almost full_ and instead insert a
`DropData` message into the stream that will provide the
`{vbucket, indexid, vbuuid, seqno}` where seqno will indicate the first
mutations that was dropped. `DropData` messages won't be inserted for subsequent
mutations that are dropped. `StreamBegin` and `StreamEnd` messages will still
be queued up until the queue gets fully filled up. This is to ensure that
StreamBegin and StreamEnd messages will never be lost due to slow connections.

Subsequently, when output queue gets emptied out, router will insert regular
mutations into the queue. On the indexer side `DropData` message will trigger a
catchup connection.

#### smart throttling

Smart throttling will selectively drop mutations based on transport statistics
between router and indexer for a given window of time. Based on statistics we
are hoping that router can detect the subset of indexer nodes that are slow,
* if the subset is small then router will insert DROP mutation, there by
  triggering catchup connection for slow indexer nodes.
* If the subset is more than say half the total number of indexer nodes,
  router won't drop mutation, leading to a slow secondary index until all the
  slow index nodes come out of its slowness.
