// Tool generates JSON document using monster productions.
// * productions are defined for `default`, `users` and `projects` bucket.
// * parallel load can be generated using `-par` switch.
// * `-count` switch specify no. of documents to be generated by each routine.

package main

import "flag"
import "fmt"
import "io/ioutil"
import "path/filepath"
import "log"
import "net/url"
import "os"
import "strings"
import "strconv"
import "time"
import "reflect"
import "unsafe"
import "math/rand"

import "github.com/couchbase/indexing/secondary/dcp"
import c "github.com/couchbase/indexing/secondary/common"
import "github.com/couchbase/indexing/secondary/logging"
import parsec "github.com/prataprc/goparsec"
import "github.com/prataprc/monster"
import mcommon "github.com/prataprc/monster/common"
import "github.com/couchbase/cbauth"

var options struct {
	seed     int // seed for monster tool
	auth     string
	buckets  []string // buckets to populate
	prods    []string
	rn       int
	un       int
	dn       int
	bagdir   string
	randkey  bool
	prefix   string
	parallel int // number of parallel routines per bucket
	count    int // number of documents to be generated per routine
	expiry   int // set expiry for the document, in seconds
	debug    bool
	verbose  bool
}

var done = make(chan bool, 16)

func argParse() string {
	var buckets, prods string
	var ratio string
	var err error

	seed := time.Now().UTC().Second()
	flag.IntVar(&options.seed, "seed", seed,
		"seed for monster tool")
	flag.StringVar(&options.auth, "auth", "",
		"Auth user and password")
	flag.StringVar(&buckets, "buckets", "default",
		"comma separated list of buckets")
	flag.StringVar(&prods, "prods", "users.prod",
		"comma separated list of production files for each bucket")
	flag.StringVar(&ratio, "ratio", "80,10,10",
		"percentage of reads, updates, deletes")
	flag.StringVar(&options.bagdir, "bagdir", "",
		"bagdirectory for production files.")
	flag.IntVar(&options.parallel, "par", 1,
		"number of parallel routines per bucket")
	flag.StringVar(&options.prefix, "prefix", "",
		"prefix to doc key")
	flag.BoolVar(&options.randkey, "randkey", false,
		"generate random key")
	flag.IntVar(&options.count, "count", 0,
		"number of documents to be generated per routine")
	flag.IntVar(&options.expiry, "expiry", 0,
		"expiry duration for a document (TTL)")
	flag.BoolVar(&options.debug, "g", false,
		"log in debug mode")
	flag.BoolVar(&options.verbose, "v", false,
		"log in verbose mode")

	flag.Parse()

	options.buckets = strings.Split(buckets, ",")
	options.prods = strings.Split(prods, ",")
	options.rn, err = strconv.Atoi(strings.Split(ratio, ",")[0])
	mf(err, "invalid ratio")
	options.un, err = strconv.Atoi(strings.Split(ratio, ",")[1])
	mf(err, "invalid ratio")
	options.dn, err = strconv.Atoi(strings.Split(ratio, ",")[2])

	// the last production file is used for remaining bucket.
	if pn, bn := len(options.prods), len(options.buckets); pn != bn {
		lastprod := options.prods[pn-1]
		for i := pn; i < bn; i++ {
			options.prods = append(options.prods, lastprod)
		}
	}

	args := flag.Args()
	if len(args) < 1 || options.bagdir == "" {
		usage()
		os.Exit(1)
	}
	return args[0]
}

func usage() {
	fmt.Fprintf(os.Stderr, "Usage : %s [OPTIONS] <cluster-addr> \n", os.Args[0])
	flag.PrintDefaults()
}

func main() {
	cluster := argParse()
	if !strings.HasPrefix(cluster, "http://") {
		cluster = "http://" + cluster
	}

	// setup cbauth
	if options.auth != "" {
		up := strings.Split(options.auth, ":")
		_, err := cbauth.InternalRetryDefaultInit(cluster, up[0], up[1])
		if err != nil {
			logging.Fatalf("Failed to initialize cbauth: %s", err)
		}
	}

	// ratio based load, will start 100 routines and dice them up.
	outch := spawnWorkers(cluster, options.rn, options.un, options.dn)

	expected, n := 0, len(options.buckets)*options.parallel*options.count
	for expected < n {
		<-outch
		expected++
	}
	fmt.Println("Done..")
}

var buckets = make(map[string]*couchbase.Bucket)

func spawnWorkers(cluster string, rn, un, dn int) chan [3]string {
	var inch, outch chan [3]string

	updatechs := make(map[string]chan [3]string)
	chsz := 10000
	gench := make(chan [3]string, chsz)
	for i, bucket := range options.buckets {
		u, err := url.Parse(cluster)
		mf(err, "parse")
		c, err := couchbase.Connect(u.String())
		mf(err, "connect - "+u.String())
		p, err := c.GetPool("default")
		mf(err, "pool")
		b, err := p.GetBucket(bucket)
		mf(err, "bucket")
		buckets[bucket] = b

		prodfile := options.prods[i]
		updatech := make(chan [3]string, chsz)
		updatechs[bucket] = updatech
		for i := 0; i < options.parallel; i++ {
			go gendocs(true, bucket, prodfile, i+1, options.count, gench)
			if un > 0 {
				updtcount := (options.count * un) / 100 * 2
				go gendocs(false, bucket, prodfile, i+1, updtcount, updatech)
			}
		}
	}

	inch = gench
	for i := 0; i < 100; i++ {
		if rn > 0 {
			outch = make(chan [3]string, chsz)
			go doRead(inch, outch)
			rn--
			inch = outch
		} else if un > 0 {
			outch = make(chan [3]string, chsz)
			go doUpdate(inch, outch, updatechs, un)
			un--
			inch = outch
		} else if dn > 0 {
			outch = make(chan [3]string, chsz)
			go doDelete(inch, outch, dn)
			dn--
			inch = outch
		}
	}
	return inch
}

func doRead(inch, outch chan [3]string) {
	counts := make(map[string]int)
	for {
		args, ok := <-inch
		if ok == false {
			break
		}
		bucketname := args[0]
		key := args[1]
		_, err := buckets[bucketname].GetRaw(key)
		mf(err, "error reading document")
		outch <- args
		if _, ok := counts[bucketname]; !ok {
			counts[bucketname] = 0
		}
		counts[bucketname]++
	}
	for bucketname, count := range counts {
		verbosef("read %v documents in %q bucket\n", count, bucketname)
	}
	close(outch)
}

func doUpdate(
	inch, outch chan [3]string,
	updatechs map[string]chan [3]string, un int) {

	counts := make(map[string]int)

	for {
		args, ok := <-inch
		if ok == false {
			break
		} else if rand.Intn(100*un) < un {
			bucketname := args[0]
			key := args[1]
			nargs := <-updatechs[bucketname]
			doc := nargs[2]
			err := buckets[bucketname].SetRaw(key, options.expiry, str2bytes(doc))
			args[2] = doc
			mf(err, "error updating document")
			outch <- args
			if _, ok := counts[bucketname]; !ok {
				counts[bucketname] = 0
			}
			counts[bucketname]++
		}
	}
	for bucketname, count := range counts {
		verbosef("updated %v documents in %q bucket\n", count, bucketname)
	}
	close(outch)
}

func doDelete(inch chan [3]string, outch chan [3]string, dn int) {
	counts := make(map[string]int)
	for {
		args, ok := <-inch
		if ok == false {
			break
		} else if rand.Intn(100*dn) < dn {
			bucketname := args[0]
			key := args[1]
			err := buckets[bucketname].Delete(key)
			mf(err, "error deleting document")
			if _, ok := counts[bucketname]; !ok {
				counts[bucketname] = 0
			}
			counts[bucketname]++
		}
		outch <- args
	}
	for bucketname, count := range counts {
		verbosef("deleted %v documents in %q bucket\n", count, bucketname)
	}
	close(outch)
}

func gendocs(
	create bool,
	bucketname, prodfile string, idx, count int, gench chan [3]string) {

	// compile
	text, err := ioutil.ReadFile(prodfile)
	if err != nil {
		log.Fatal(err)
	}
	scope := compile(parsec.NewScanner(text)).(mcommon.Scope)
	seed, bagdir := uint64(options.seed), options.bagdir
	scope = monster.BuildContext(scope, seed, bagdir, prodfile)
	nterms := scope["_nonterminals"].(mcommon.NTForms)
	// evaluate
	for i := 0; i < count; i++ {
		scope = scope.RebuildContext()
		doc := evaluate("root", scope, nterms["s"]).(string)
		key := makeKey(prodfile, idx, i+1)
		buckets[bucketname].SetRaw(key, options.expiry, str2bytes(doc))
		gench <- [3]string{bucketname, key, doc}
	}
	fmsg := "generated %v docs for bucket %v, routine %v\n"
	verbosef(fmsg, count, bucketname, idx)
}

func compile(s parsec.Scanner) parsec.ParsecNode {
	defer func() {
		if r := recover(); r != nil {
			log.Printf("%v at %v", r, s.GetCursor())
		}
	}()
	root, _ := monster.Y(s)
	return root
}

func evaluate(
	name string, scope mcommon.Scope, forms []*mcommon.Form) interface{} {

	defer func() {
		if r := recover(); r != nil {
			log.Printf("%v", r)
		}
	}()
	return monster.EvalForms(name, scope, forms)
}

func mf(err error, msg string) {
	if err != nil {
		log.Fatalf("%v: %v", msg, err)
	}
}

func makeKey(prodfile string, idx, i int) string {
	fname := filepath.Base(prodfile)
	if options.randkey {
		uuid, _ := c.NewUUID()
		return fmt.Sprintf("%v-%s-%s-%v-%v", options.prefix, fname, uuid.Str(), idx, i+1)
	}
	return fmt.Sprintf("%v-%s-%v-%v", options.prefix, fname, idx, i+1)
}

func bytes2str(bytes []byte) string {
	if bytes == nil {
		return ""
	}
	sl := (*reflect.SliceHeader)(unsafe.Pointer(&bytes))
	st := &reflect.StringHeader{Data: sl.Data, Len: sl.Len}
	return *(*string)(unsafe.Pointer(st))
}

func str2bytes(str string) []byte {
	if str == "" {
		return nil
	}
	st := (*reflect.StringHeader)(unsafe.Pointer(&str))
	sl := &reflect.SliceHeader{Data: st.Data, Len: st.Len, Cap: st.Len}
	return *(*[]byte)(unsafe.Pointer(sl))
}

func verbosef(fmsg string, args ...interface{}) {
	if options.verbose || options.debug {
		fmt.Printf(fmsg, args...)
	}
}

func debugf(fmsg string, args ...interface{}) {
	if options.debug {
		fmt.Printf(fmsg, args...)
	}
}
