// @copyright 2022-Present Couchbase, Inc.
//
// Use of this software is governed by the Business Source License included
// in the file licenses/BSL-Couchbase.txt.  As of the Change Date specified
// in that file, in accordance with the Business Source License, use of this
// software will be governed by the Apache License, Version 2.0, included in
// the file licenses/APL2.txt.

package indexer

import (
	"encoding/json"
	"errors"
	"fmt"
	"io/ioutil"
	"strings"
	"sync"
	"time"

	"github.com/couchbase/cbauth/metakv"
	"github.com/couchbase/cbauth/service"
	"github.com/couchbase/indexing/secondary/common"
	"github.com/couchbase/indexing/secondary/logging"
	"github.com/couchbase/indexing/secondary/manager"
	"github.com/couchbase/plasma"
)

////////////////////////////////////////////////////////////////////////////////////////////////////
// PauseUploadToken and PauseUploadState
// Used to convey state change information between Pause master and follower nodes.
////////////////////////////////////////////////////////////////////////////////////////////////////

type PauseUploadState byte

const (
	// Posted is the initial state as generated by master. Indicates to followers to start upload work.
	// Followers change state to InProgress.
	PauseUploadTokenPosted PauseUploadState = iota

	// InProgess indicates that followers are actually performing the upload work. In addition, any
	// upload work to be done only by master is also carried out. Once all the work is done, followers
	// change the state to Processed.
	PauseUploadTokenInProgess

	// Processed indicates that for a follower, all the upload work is completed. Master will delete the token
	// and once all followers are done, cleanup is initiated.
	PauseUploadTokenProcessed

	// Error indicates that for a follower, an error was encountered during upload. Master will initiate cleanup.
	PauseUploadTokenError
)

func (s PauseUploadState) String() string {
	switch s {
	case PauseUploadTokenPosted:
		return "PauseUploadTokenPosted"
	case PauseUploadTokenInProgess:
		return "PauseUploadTokenInProgess"
	case PauseUploadTokenProcessed:
		return "PauseUploadTokenProcessed"
	case PauseUploadTokenError:
		return "PauseUploadTokenError"
	}

	return fmt.Sprintf("PauseUploadState-UNKNOWN-STATE-[%v]", byte(s))
}

const PauseUploadTokenTag = "PauseUploadToken"
const PauseUploadTokenPathPrefix = PauseMetakvDir + PauseUploadTokenTag

type PauseUploadToken struct {
	MasterId   string
	FollowerId string
	PauseId    string
	State      PauseUploadState
	BucketName string
	Error      string
	Shards     []common.ShardId
}

func newPauseUploadToken(masterUuid, followerUuid, pauseId, bucketName string) (string, *PauseUploadToken, error) {
	put := &PauseUploadToken{
		MasterId:   masterUuid,
		FollowerId: followerUuid,
		PauseId:    pauseId,
		State:      PauseUploadTokenPosted,
		BucketName: bucketName,
	}

	ustr, err := common.NewUUID()
	if err != nil {
		logging.Warnf("newPauseUploadToken: Failed to generate uuid: err[%v]", err)
		return "", nil, err
	}

	putId := fmt.Sprintf("%s%s", PauseUploadTokenTag, ustr.Str())

	return putId, put, nil
}

func decodePauseUploadToken(path string, value []byte) (string, *PauseUploadToken, error) {

	putIdPos := strings.Index(path, PauseUploadTokenTag)
	if putIdPos < 0 {
		return "", nil, fmt.Errorf("PauseUploadTokenTag[%v] not present in metakv path[%v]",
			PauseUploadTokenTag, path)
	}

	putId := path[putIdPos:]

	put := &PauseUploadToken{}
	err := json.Unmarshal(value, put)
	if err != nil {
		logging.Errorf("decodePauseUploadToken: Failed to unmarshal value[%s] path[%v]: err[%v]",
			string(value), path, err)
		return "", nil, err
	}

	return putId, put, nil
}

func (put *PauseUploadToken) Clone() *PauseUploadToken {
	put1 := *put
	put2 := put1
	return &put2
}

func setPauseUploadTokenInMetakv(putId string, put *PauseUploadToken) {

	rhCb := func(r int, err error) error {
		if r > 0 {
			logging.Warnf("setPauseUploadTokenInMetakv::rhCb: err[%v], Retrying[%d]", err, r)
		}

		return common.MetakvSet(PauseMetakvDir+putId, put)
	}

	rh := common.NewRetryHelper(10, time.Second, 1, rhCb)
	err := rh.Run()

	if err != nil {
		logging.Fatalf("setPauseUploadTokenInMetakv: Failed to set PauseUploadToken In Meta Storage:" +
			" putId[%v] put[%v] err[%v]", putId, put, err)
		common.CrashOnError(err)
	}
}

////////////////////////////////////////////////////////////////////////////////////////////////////
// Pauser class - Perform the Pause of a given bucket (similar to Rebalancer's role).
// This is used only on the master node of a task_PAUSE task to do the GSI orchestration.
////////////////////////////////////////////////////////////////////////////////////////////////////

//  Called at the end of the pause lifecycle. It takes pauseId and any error as input.
type PauseDoneCallback func(string, error)

// Pauser object holds the state of Pause orchestration
type Pauser struct {
	// nodeDir is "node_<nodeId>/" for this node, where nodeId is the 32-digit hex ID from ns_server
	nodeDir string

	// otherIndexAddrs is "host:port" to all the known Index Service nodes EXCLUDING this one
	otherIndexAddrs []string

	// pauseMgr is the singleton parent of this object
	pauseMgr *PauseServiceManager

	// task is the task_PAUSE task we are executing (protected by task.taskMu). It lives in the
	// pauseMgr.tasks map (protected by pauseMgr.tasksMu). Only the current object should change or
	// delete task at this point, but GetTaskList and other processing may concurrently read it.
	// Thus Pauser needs to write lock task.taskMu for changes but does not need to read lock it.
	task *taskObj

	// Channels used for signalling
	// Used to signal that the PauseUploadTokens have been published.
	waitForTokenPublish chan struct{}
	// Used to signal metakv observer to stop
	metakvCancel chan struct{}

	metakvMutex sync.RWMutex
	wg          sync.WaitGroup

	// Global token associated with this Pause task
	pauseToken *PauseToken

	// in-memory bookkeeping for observed tokens
	masterTokens, followerTokens map[string]*PauseUploadToken

	// lock protecting access to maps like masterTokens and followerTokens
	mu sync.RWMutex

	// For cleanup
	retErr      error
	cleanupOnce sync.Once
	doneCb      PauseDoneCallback
}

// NewPauser creates a Pauser instance to execute the given task. It saves a pointer to itself in
// task.pauser (visible to pauseMgr parent) and launches a goroutine for the work.
//
//	pauseMgr - parent object (singleton)
//	task - the task_PAUSE task this object will execute
//	pauseToken - global PauseToken
//	doneCb - callback that initiates the cleanup phase
func NewPauser(pauseMgr *PauseServiceManager, task *taskObj, pauseToken *PauseToken,
	doneCb PauseDoneCallback) *Pauser {

	pauser := &Pauser{
		pauseMgr: pauseMgr,
		task:     task,
		nodeDir:  "node_" + string(pauseMgr.genericMgr.nodeInfo.NodeID) + "/",

		waitForTokenPublish: make(chan struct{}),
		metakvCancel:        make(chan struct{}),
		pauseToken:          pauseToken,

		followerTokens: make(map[string]*PauseUploadToken),

		doneCb: doneCb,
	}

	task.taskMu.Lock()
	task.pauser = pauser
	task.taskMu.Unlock()

	return pauser
}

////////////////////////////////////////////////////////////////////////////////////////////////////
// Methods
////////////////////////////////////////////////////////////////////////////////////////////////////

func (p *Pauser) startWorkers() {
	go p.observePause()

	if p.task.isMaster() {
		go p.initPauseAsync()
	} else {
		// if not master, no need to wait for publishing of tokens
		close(p.waitForTokenPublish)
	}
}

func (p *Pauser) initPauseAsync() {

	// TODO: init progress update

	// Generate PauseUploadTokens
	puts, err := p.generatePauseUploadTokens()
	if err != nil {
		logging.Errorf("Pauser::initPauseAsync: Failed to generate PauseUploadTokens: err[%v], puts[%v]",
			err, puts)

		p.finishPause(err)
		return
	}

	// Initilize master tokens instead of waiting to observe them through metakv callback
	p.masterTokens = puts

	// Publish tokens to metaKV
	// will crash if cannot set in metaKV even after retries.
	p.publishPauseUploadTokens(puts)

	// Ask observe to continue
	close(p.waitForTokenPublish)
}

func (p *Pauser) generatePauseUploadTokens() (map[string]*PauseUploadToken, error) {
	indexerUuids, err := p.getIndexerUuids()
	if err != nil || len(indexerUuids) < 1 {
		logging.Errorf("Pauser::generatePauseUploadTokens: Error getting indexer node UUIDs: err[%v]" +
			" indexerUuids[%v]", err, indexerUuids)
		return nil, err
	}

	puts := make(map[string]*PauseUploadToken)
	nodeUUID := string(p.pauseMgr.nodeInfo.NodeID)

	for _, uuid := range indexerUuids {
		rhCb := func(retryAttempt int, lastErr error) error {
			putId, put, err := newPauseUploadToken(nodeUUID, uuid, p.task.taskId, p.task.bucket)
			if err != nil {
				logging.Warnf("Pauser::generatePauseUploadTokens::rhCb: Error making new PauseUploadToken: " +
					"err[%v] retryAttempt[%d] lastErr[%v]", err, retryAttempt, lastErr)
				return err
			}

			if oldPut, ok := puts[putId]; ok {
				err = fmt.Errorf("put with putId[%s] already exists, put[%v]", putId, oldPut)
				logging.Warnf("Pauser::generatePauseUploadTokens::rhCb: PUTId Collision: " +
					"err[%v] retryAttempt[%d] lastErr[%v]", err, retryAttempt, lastErr)
				return err
			}

			puts[putId] = put

			return nil
		}

		rh := common.NewRetryHelper(10, 100 * time.Millisecond, 1, rhCb)
		if err := rh.Run(); err != nil {
			logging.Errorf("Pauser::generatePauseUploadTokens: Failed to generate PauseUploadToken: err[%v]",
				err)
			return nil, err
		}
	}

	return puts, nil
}

func (p *Pauser) getIndexerUuids() (indexerUuids []string, err error) {

	p.pauseMgr.genericMgr.cinfo.Lock()
	defer p.pauseMgr.genericMgr.cinfo.Unlock()

	if err := p.pauseMgr.genericMgr.cinfo.FetchNodesAndSvsInfo(); err != nil {
		logging.Errorf("Pauser::getIndexerUuids Error Fetching Cluster Information %v", err)
		return nil, err
	}

	nids := p.pauseMgr.genericMgr.cinfo.GetNodeIdsByServiceType(common.INDEX_HTTP_SERVICE)
	url := "/nodeuuid"

	for _, nid := range nids {
		haddr, err := p.pauseMgr.genericMgr.cinfo.GetServiceAddress(nid, common.INDEX_HTTP_SERVICE, true)
		if err != nil {
			return nil, err
		}

		resp, err := getWithAuth(haddr + url)
		if err != nil {
			logging.Errorf("Pauser::getIndexerUuids Unable to Fetch Node UUID %v %v", haddr, err)
			return nil, err

		} else {
			defer resp.Body.Close()

			bytes, _ := ioutil.ReadAll(resp.Body)
			uuid := string(bytes)
			indexerUuids = append(indexerUuids, uuid)
		}
	}

	return indexerUuids, nil
}

func (p *Pauser) publishPauseUploadTokens(puts map[string]*PauseUploadToken) {
	for putId, put := range puts {
		setPauseUploadTokenInMetakv(putId, put)
		logging.Infof("Pauser::publishPauseUploadTokens Published pause upload token: %v", putId)
	}
}

func (p *Pauser) observePause() {
	logging.Infof("Pauser::observePause pauseToken[%v] master[%v]", p.pauseToken, p.task.isMaster())

	<-p.waitForTokenPublish

	err := metakv.RunObserveChildren(PauseMetakvDir, p.processUploadTokens, p.metakvCancel)
	if err != nil {
		logging.Errorf("Pauser::observePause Exiting on metaKV observe: err[%v]", err)

		p.finishPause(err)
	}

	logging.Infof("Pauser::observePause exiting: err[%v]", err)
}

// processUploadTokens is metakv callback, not intended to be called otherwise
func (p *Pauser) processUploadTokens(kve metakv.KVEntry) error {

	if kve.Path == buildMetakvPathForPauseToken(p.pauseToken.PauseId) {
		// Process PauseToken

		logging.Infof("Pauser::processUploadTokens: PauseToken path[%v] value[%s]", kve.Path, kve.Value)

		if kve.Value == nil {
			// During cleanup, PauseToken is deleted by master and serves as a signal for
			// all observers on followers to stop.

			logging.Infof("Pauser::processUploadTokens: PauseToken Deleted. Mark Done.")
			p.cancelMetakv()
			p.finishPause(nil)
		}

	} else if strings.Contains(kve.Path, PauseUploadTokenPathPrefix) {
		// Process PauseUploadTokens

		if kve.Value != nil {
			putId, put, err := decodePauseUploadToken(kve.Path, kve.Value)
			if err != nil {
				logging.Errorf("Pauser::processUploadTokens: Failed to decode PauseUploadToken. Ignored.")
				return nil
			}

			p.processPauseUploadToken(putId, put)

		} else {
			logging.Infof("Pauser::processUploadTokens: Received empty or deleted PauseUploadToken path[%v]",
				kve.Path)

		}
	}

	return nil
}

func (p *Pauser) cancelMetakv() {
	p.metakvMutex.Lock()
	defer p.metakvMutex.Unlock()

	if p.metakvCancel != nil {
		close(p.metakvCancel)
		p.metakvCancel = nil
	}
}

func (p *Pauser) processPauseUploadToken(putId string, put *PauseUploadToken) {
	logging.Infof("Pauser::processPauseUploadToken putId[%v] put[%v]", putId, put)
	if !p.addToWaitGroup() {
		logging.Errorf("Pauser::processPauseUploadToken: Failed to add to pauser waitgroup.")
		return
	}

	defer p.wg.Done()

	// "processed" var ensures only the incoming token state gets processed by this
	// call, as metakv will call parent processUploadTokens again for each state change.
	var processed bool

	nodeUUID := string(p.pauseMgr.nodeInfo.NodeID)

	if put.MasterId == nodeUUID {
		processed = p.processPauseUploadTokenAsMaster(putId, put)
	}

	if (put.FollowerId == nodeUUID && !processed) {
		p.processPauseUploadTokenAsFollower(putId, put)
	}
}

func (p *Pauser) addToWaitGroup() bool {
	p.metakvMutex.Lock()
	defer p.metakvMutex.Unlock()

	if p.metakvCancel != nil {
		p.wg.Add(1)
		return true
	}
	return false
}

func (p *Pauser) processPauseUploadTokenAsMaster(putId string, put *PauseUploadToken) bool {

	logging.Infof("Pauser::processPauseUploadTokenAsMaster: putId[%v] put[%v]", putId, put)

	if put.PauseId != p.task.taskId {
		logging.Warnf("Pauser::processPauseUploadTokenAsMaster: Found PauseUploadToken[%v] with Unknown " +
			"PauseId. Expected to match local taskId[%v]", put, p.task.taskId)

		return true
	}

	if put.Error != "" {
		logging.Errorf("Pauser::processPauseUploadTokenAsMaster: Detected PauseUploadToken[%v] in Error state." +
			" Abort.", put)

		p.cancelMetakv()
		go p.finishPause(errors.New(put.Error))

		return true
	}

	if !p.checkValidNotifyState(putId, put, "master") {
		return true
	}

	switch put.State {

	case PauseUploadTokenPosted:
		// Follower owns token, do nothing

		return false

	case PauseUploadTokenInProgess:
		// Follower owns token, just mark in memory maps.

		p.updateInMemToken(putId, put, "master")
		return false

	case PauseUploadTokenProcessed:
		// Master owns token

		shardPaths := put.Shards
		if shardPaths != nil {
			func() {
				p.task.pauseMetadata.lock.Lock()
				defer p.task.pauseMetadata.lock.Unlock()
				for _, shardId := range shardPaths {
					p.task.pauseMetadata.addShardNoLock(service.NodeID(put.FollowerId), shardId)
				}
			}()
		}

		// Follower completed work, delete token
		err := common.MetakvDel(PauseMetakvDir + putId)
		if err != nil {
			logging.Fatalf("Pauser::processPauseUploadTokenAsMaster: Failed to delete PauseUploadToken[%v] with" +
				" putId[%v] In Meta Storage: err[%v]", put, putId, err)
			common.CrashOnError(err)
		}

		p.updateInMemToken(putId, put, "master")

		if p.checkAllTokensDone() {
			// All the followers completed work

			// TODO: set progress 100%

			logging.Infof("Pauser::processPauseUploadTokenAsMaster: No Tokens Found. Mark Done.")

			err := p.masterUploadPauseMetadata()

			p.cancelMetakv()

			go p.finishPause(err)
		}

		return true

	default:
		return false

	}

}

func (p *Pauser) finishPause(err error) {

	if p.retErr == nil && err != nil {
		p.retErr = err
	}

	p.cleanupOnce.Do(p.doFinish)
}

func (p *Pauser) doFinish() {
	logging.Infof("Pauser::doFinish Cleanup: retErr[%v]", p.retErr)

	// TODO: signal others that we are cleaning up using done channel

	p.Cleanup()
	p.wg.Wait()

	// TODO: move this to inside the done callback

	// call done callback to start the cleanup phase
	p.doneCb(p.pauseToken.PauseId, p.retErr)
}

func (p *Pauser) processPauseUploadTokenAsFollower(putId string, put *PauseUploadToken) bool {

	logging.Infof("Pauser::processPauseUploadTokenAsFollower: putId[%v] put[%v]", putId, put)

	if put.PauseId != p.task.taskId {
		logging.Warnf("Pauser::processPauseUploadTokenAsFollower: Found PauseUploadToken[%v] with Unknown " +
			"PauseId. Expected to match local taskId[%v]", put, p.task.taskId)

		return true
	}

	if !p.checkValidNotifyState(putId, put, "follower") {
		return true
	}

	switch put.State {

	case PauseUploadTokenPosted:
		// Follower owns token, update in-memory and move to InProgress State

		p.updateInMemToken(putId, put, "follower")

		put.State = PauseUploadTokenInProgess
		setPauseUploadTokenInMetakv(putId, put)

		return true

	case PauseUploadTokenInProgess:
		// Follower owns token, update in-memory and start pause work

		p.updateInMemToken(putId, put, "follower")

		go p.startPauseUpload(putId, put)

		return true

	case PauseUploadTokenProcessed:
		// Master owns token, follower work is done cleanup and start watcher

		p.updateInMemToken(putId, put, "follower")

		p.finishPause(nil)

		return false

	default:
		return false
	}
}

func (p *Pauser) startPauseUpload(putId string, put *PauseUploadToken) {
	start := time.Now()
	logging.Infof("Pauser::startPauseUpload: Begin work: putId[%v] put[%v]", putId, put)
	defer logging.Infof("Pauser::startPauseUpload: Done work: putId[%v] put[%v] took[%v]",
		putId, put, time.Since(start))

	if !p.addToWaitGroup() {
		logging.Errorf("Pauser::startPauseUpload: Failed to add to pauser waitgroup.")
		return
	}
	defer p.wg.Done()

	shardPaths, err := p.followerUploadBucketData()
	if err != nil {
		put.Error = err.Error()
	} else if shardPaths != nil {
		shardIds := make([]common.ShardId, 0, len(shardPaths))
		for shardId := range shardPaths {
			shardIds = append(shardIds, shardId)
		}
		put.Shards = shardIds
	}

	// work done, change state, master handler will pick it up and do cleanup.
	put.State = PauseUploadTokenProcessed
	setPauseUploadTokenInMetakv(putId, put)
}

// Often, metaKV can send multiple notifications for the same state change (probably
// due to the eventual consistent nature of metaKV). Keep track of all state changes
// in in-memory bookkeeping and ignore the duplicate notifications
func (p *Pauser) checkValidNotifyState(putId string, put *PauseUploadToken, caller string) bool {

	// As the default state is "PauseUploadTokenPosted"
	// do not check for valid state changes for this state
	if put.State == PauseUploadTokenPosted {
		return true
	}

	p.mu.RLock()
	defer p.mu.RUnlock()

	var inMemToken *PauseUploadToken
	var ok bool

	if caller == "master" {
		inMemToken, ok = p.masterTokens[putId]
	} else if caller == "follower" {
		inMemToken, ok = p.followerTokens[putId]
	}

	if ok {
		// Token seen before, validate the state

		// < for invalid state change
		// == for duplicate notification
		if put.State <= inMemToken.State {
			logging.Warnf("Pauser::checkValidNotifyState Detected Invalid State Change Notification" +
				" for [%v]. putId[%v] Local[%v] Metakv[%v]", caller, putId, inMemToken.State, put.State)

			return false
		}
	}

	return true
}

func (p *Pauser) updateInMemToken(putId string, put *PauseUploadToken, caller string) {

	p.mu.Lock()
	defer p.mu.Unlock()

	if caller == "master" {
		p.masterTokens[putId] = put.Clone()
	} else if caller == "follower" {
		p.followerTokens[putId] = put.Clone()
	}
}

func (p *Pauser) checkAllTokensDone() bool {

	p.mu.Lock()
	defer p.mu.Unlock()

	for putId, put := range p.masterTokens {
		if put.State < PauseUploadTokenProcessed {
			// Either posted or processing

			logging.Infof("Pauser::checkAllTokensDone PauseUploadToken: putId[%v] is in state[%v]",
				putId, put.State)

			return false
		}
	}

	return true
}

// restGetLocalIndexMetadataBinary calls the /getLocalndexMetadata REST API (request_handler.go) via
// self-loopback to get the index metadata for the current node and the task's bucket (tenant). This
// verifies it can be unmarshaled, but it returns a checksummed and optionally compressed byte slice
// version of the data rather than the unmarshaled object.
func (this *Pauser) restGetLocalIndexMetadataBinary(compress bool) ([]byte, *manager.LocalIndexMetadata, error) {
	const _restGetLocalIndexMetadataBinary = "Pauser::restGetLocalIndexMetadataBinary:"

	url := fmt.Sprintf("%v/getLocalIndexMetadata?useETag=false&bucket=%v",
		this.pauseMgr.httpAddr, this.task.bucket)
	resp, err := getWithAuth(url)
	if err != nil {
		return nil, nil, err
	}
	defer resp.Body.Close()

	byteSlice, err := ioutil.ReadAll(resp.Body)
	if err != nil {
		return nil, nil, err
	}

	// Verify response can be unmarshaled
	metadata := new(manager.LocalIndexMetadata)
	err = json.Unmarshal(byteSlice, metadata)
	if err != nil {
		return nil, nil, err
	}
	if len(metadata.IndexDefinitions) == 0 {
		return nil, nil, nil
	}

	// Return checksummed and optionally compressed byte slice, not the unmarshaled object
	return common.ChecksumAndCompress(byteSlice, compress), metadata, nil
}

// masterUploadPauseMetadata is master's method to upload PauseMetadata to object store
//
// this method should be called after all the followers have finished execution so we can
// correctly upload the metadata about all the follower nodes
//
// meta about follower nodes should be gathered before calling this function
// object structure after upload:
// archivePath/
// └── pauseMetadata.json
func (p *Pauser) masterUploadPauseMetadata() error {
	metadata := p.task.pauseMetadata

	metadata.lock.Lock()
	metadata.setVersionNoLock(common.GetLocalInternalVersion().String())
	metadata.lock.Unlock()

	data, err := json.Marshal(metadata)
	if err != nil {
		logging.Errorf("Pauser::masterUploadPauseMetadata: couldn't marshal pause metadata err :%v  for task ID: %v.", err, p.task.taskId)
		return err
	}

	cfg := p.pauseMgr.config.Load()
	cfgValue, ok := cfg["pause_resume.compression"]
	var compression bool
	if !ok {
		compression = true
	} else {
		compression = cfgValue.Bool()
	}
	if !compression {
		logging.Infof("Pauser::masterUploadPauseMetadata: compression is disabled. will upload raw data")
	}

	data = common.ChecksumAndCompress(data, compression)

	ctx := p.task.ctx
	plasmaCfg := plasma.DefaultConfig()

	copier := plasma.MakeFileCopier(p.task.archivePath,"",plasmaCfg.Environment,plasmaCfg.CopyConfig)
	if copier == nil {
		err = fmt.Errorf("couldn't create copier object. archive path %v is unsupported",p.task.archivePath)
		logging.Errorf("Pauser::masterUploadPauseMetadata: %v",err)
		return err
	}

	url, err := copier.GetPathEncoding(fmt.Sprintf("%v%v",p.task.archivePath,FILENAME_PAUSE_METADATA))
	if err != nil {
		logging.Errorf("Pauser::masterUploadPauseMetadata: url encoding failed, err: %v to %v%v for task ID: %v", err, p.task.archivePath, FILENAME_METADATA, p.task.taskId)
		return err
	}
	_, err = copier.UploadBytes(ctx,data,url)

	if err != nil {
		logging.Errorf("Pauser::masterUploadPauseMetadata: upload of pause metadata failed to path : %v%v err: %v for task ID %v", p.task.archivePath, FILENAME_PAUSE_METADATA, err, p.task.taskId)
		return err
	}
	logging.Infof("Pauser::masterUploadPauseMetadata: successful upload of pause metadata to %v%v for task ID %v", p.task.archivePath, FILENAME_PAUSE_METADATA, p.task.taskId)
	return nil
}

// followerUploadBucketData is follower's method to upload bucket related data to object store
//
// it gathers bucket's indexes and uploads:
// * Index Metadata -> FILENAME_METADATA
// * Index Stats -> FILENAME_STATS
// * Start Plasma Shard Transfer
// object store structure after upload:
// archivePath/
// └── node_<nodeId>/
//		├── indexMetadata.json
//		├── indexStats.json
//		└── /plasma_storage/PauseResume/<bucketName>/<shardId>/
//			└── plasma shard data
func (p *Pauser) followerUploadBucketData() (map[common.ShardId]string, error) {
	nodePath := p.task.archivePath + p.nodeDir
	cfg := p.pauseMgr.config.Load()
	cfgValue, ok := cfg["pause_resume.compression"]
	var compression bool
	if !ok {
		compression = true
	} else {
		compression = cfgValue.Bool()
	}
	if !compression {
		logging.Infof("Pauser::followerUploadBucketData: compression is disabled. will upload raw data")
	}

	plasmaCfg := plasma.DefaultConfig()
	copier := plasma.MakeFileCopier(p.task.archivePath,"",plasmaCfg.Environment,plasmaCfg.CopyConfig)
	if copier == nil {
		err := fmt.Errorf("couldn't create a copier object. archive path %v is unsupported", p.task.archivePath)
		logging.Errorf("Pauser::followerUploadBucketData: %v", err)
		return nil, err
	}
	ctx := p.task.ctx

	logging.Tracef("Pauser::followerUploadBucketData: uploading data to path %v", nodePath)

	// Step 1. Gather bucket's indexes data
	byteSlice, indexMetadata, err := p.restGetLocalIndexMetadataBinary(compression)
	if err != nil {
		logging.Errorf("Pauser::followerUploadBucketData: failed to get local index metadata err :%v for task ID: %v bucket: %v", err, p.task.taskId, p.task.bucket)
		return nil, err
	}
	if byteSlice == nil {
		// there are no indexes on this node for bucket. pause is a no-op
		logging.Infof("Pauser::followerUploadBucketData: pause is a no-op for bucket %v task ID %v", p.task.bucket, p.task.taskId)
		return nil, nil
	}

	// Step 2. Upload index metadata
	url, err := copier.GetPathEncoding(fmt.Sprintf("%v%v",nodePath,FILENAME_METADATA))
	if err != nil {
		logging.Errorf("Pauser::followerUploadBucketData: url encoding failed, err: %v to %v%v for task ID: %v", err, nodePath, FILENAME_METADATA, p.task.taskId)
		return nil, err
	}
	_, err = copier.UploadBytes(ctx,byteSlice,url)
	if err != nil {
		logging.Errorf("Pauser::followerUploadBucketData: metadata upload failed, err: %v to %v%v for task ID: %v", err, nodePath, FILENAME_METADATA, p.task.taskId)
		return nil, err
	}
	logging.Infof("Pauser::followerUploadBucketData: metadata successfully uploaded to %v%v for taskId %v", nodePath, FILENAME_METADATA, p.task.taskId)

	// Step 3. Gather index stats
	getIndexInstanceIds := func() []common.IndexInstId {
		res := make([]common.IndexInstId, 0, len(indexMetadata.IndexDefinitions))
		for _, topology := range indexMetadata.IndexTopologies {
			for _, indexDefn := range topology.Definitions {
				res = append(res, common.IndexInstId(indexDefn.Instances[0].InstId))
			}
		}
		logging.Tracef("Pauser::followerUploadBucketData::getIndexInstanceId: index instance ids: %v for bucket %v", res, p.task.bucket)
		return res
	}

	byteSlice, err = p.pauseMgr.genericMgr.statsMgr.GetStatsForIndexesToBePersisted(getIndexInstanceIds(), compression)
	if err != nil {
		logging.Errorf("Pauser::followerUploadBucketData: couldn't get stats for indexes err: %v for task ID: %v", err, p.task.taskId)
		return nil, err
	}

	// Step 4. Upload index stats
	url, err = copier.GetPathEncoding(fmt.Sprintf("%v%v",nodePath,FILENAME_STATS))
	if err != nil {
		logging.Errorf("Pauser::followerUploadBucketData: url encoding failed, err: %v to %v%v for task ID: %v", err, nodePath, FILENAME_METADATA, p.task.taskId)
		return nil, err
	}
	_, err = copier.UploadBytes(ctx,byteSlice,url)
	if err != nil {
		logging.Errorf("Pauser::followerUploadBucketData: stats upload failed, err: %v to %v%v for task ID: %v", err, nodePath, FILENAME_STATS, p.task.taskId)
		return nil, err
	}

	logging.Infof("Pauser::followerUploadBucketData: stats successfully uploaded to %v%v for taskId %v", nodePath, FILENAME_STATS, p.task.taskId)

	// Step 5. Initiate plasma shard transfer
	getShardIds := func() []common.ShardId {
		uniqueShardIds := make(map[common.ShardId]bool)
		for _, topology := range indexMetadata.IndexTopologies {
			for _, indexDefn := range topology.Definitions {
				for _, instance := range indexDefn.Instances {
					for _, partition := range instance.Partitions {
						for _, shard := range partition.ShardIds {
							uniqueShardIds[shard] = true
						}
					}
				}
			}
		}

		shardIds := make([]common.ShardId, 0, len(uniqueShardIds))
		for shardId := range uniqueShardIds {
			shardIds = append(shardIds, shardId)
		}
		logging.Tracef("Pauser::followerUploadBucketData::getShardIds: found shard Ids %v for bucket %v", shardIds, p.task.bucket)

		return shardIds
	}

	// TODO: add contextWithCancel to task and reuse it here
	closeCh := p.task.ctx.Done()
	shardPaths, err := p.pauseMgr.copyShardsWithLock(getShardIds(), p.task.taskId, p.task.bucket, nodePath, closeCh)
	if err != nil {
		return nil, err
	}
	return shardPaths, nil
}

// cleanupNoLocks stops any ongoing operation and starts bucket endpoint watchers
func (p *Pauser) cleanupNoLocks() {
	p.task.cancelNoLock()
	go monitorBucketForPauseResume(p.task.bucket, true)
}

func (p *Pauser) Cleanup() {
	p.task.taskMu.Lock()
	p.cleanupNoLocks()
	p.task.taskMu.Unlock()
	p.cancelMetakv()
}
